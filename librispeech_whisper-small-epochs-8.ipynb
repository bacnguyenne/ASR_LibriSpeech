{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11207649,"sourceType":"datasetVersion","datasetId":6987209},{"sourceId":310663,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":263466,"modelId":284573}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch torchaudio torchvision evaluate jiwer openai-whisper --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:43.066289Z","iopub.execute_input":"2025-03-31T07:15:43.066586Z","iopub.status.idle":"2025-03-31T07:15:46.469218Z","shell.execute_reply.started":"2025-03-31T07:15:43.066564Z","shell.execute_reply":"2025-03-31T07:15:46.468220Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torchaudio\nimport os\nfrom pathlib import Path\nfrom transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperForConditionalGeneration\nfrom tqdm.auto import tqdm \nimport pickle\nimport jiwer \nimport numpy as np ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:46.470549Z","iopub.execute_input":"2025-03-31T07:15:46.470892Z","iopub.status.idle":"2025-03-31T07:15:46.475989Z","shell.execute_reply.started":"2025-03-31T07:15:46.470855Z","shell.execute_reply":"2025-03-31T07:15:46.475212Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Config ","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/librispeech-train-clean-100/\" \n# MODEL_NAME = \"openai/whisper-small.en\"\nMODEL_NAME = \"/kaggle/input/whisper-small-epochs-8_librispeech-100h/pytorch/default/1/whisper-small-epochs-8\"\nSAVE_TO = \"/kaggle/working/models/\"\nSAVE_EVERY = 3000\nGRADIENT_ACCUMULATION_STEPS = 64\nEVALUATION_SIZE = 150 \nLEARNING_RATE = 1e-4\nEPOCHS = 1\nBATCH_SIZE = 16 \nTARGET_SAMPLE_RATE = 16000\nCHECKPOINT_EVERY_N_EPOCHS = 2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:15:46.477565Z","iopub.execute_input":"2025-03-31T07:15:46.477787Z","iopub.status.idle":"2025-03-31T07:15:46.495222Z","shell.execute_reply.started":"2025-03-31T07:15:46.477761Z","shell.execute_reply":"2025-03-31T07:15:46.494419Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:46.496402Z","iopub.execute_input":"2025-03-31T07:15:46.496623Z","iopub.status.idle":"2025-03-31T07:15:46.510578Z","shell.execute_reply.started":"2025-03-31T07:15:46.496604Z","shell.execute_reply":"2025-03-31T07:15:46.509824Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Load Feature Extractor and Tokenizer","metadata":{}},{"cell_type":"code","source":"feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\ntokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=\"english\", task=\"transcribe\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:46.511394Z","iopub.execute_input":"2025-03-31T07:15:46.511612Z","iopub.status.idle":"2025-03-31T07:15:46.765700Z","shell.execute_reply.started":"2025-03-31T07:15:46.511594Z","shell.execute_reply":"2025-03-31T07:15:46.764838Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"def load_librispeech_item(fileid: str, path: str, ext_audio: str, ext_txt: str):\n    speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n\n    file_text = f\"{speaker_id}-{chapter_id}{ext_txt}\"\n    file_text = os.path.join(path, speaker_id, chapter_id, file_text)\n\n    fileid_audio = f\"{speaker_id}-{chapter_id}-{utterance_id}\"\n    file_audio = f\"{fileid_audio}{ext_audio}\"\n    file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n\n    waveform, sample_rate = torchaudio.load(file_audio)\n    if sample_rate != TARGET_SAMPLE_RATE:\n        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n        waveform = resampler(waveform)\n\n    if waveform.shape[0] > 1:\n        waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n    waveform = waveform.squeeze(0)\n    transcript = None\n    with open(file_text) as ft:\n        for line in ft:\n            fileid_text, text = line.strip().split(\" \", 1)\n            if fileid_audio == fileid_text:\n                transcript = text\n                break\n        else:\n            raise FileNotFoundError(f\"Translation not found for {fileid_audio} in {file_text}\")\n\n    return {\"waveform\": waveform.numpy(), \"transcript\": transcript}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:46.766577Z","iopub.execute_input":"2025-03-31T07:15:46.766881Z","iopub.status.idle":"2025-03-31T07:15:46.772867Z","shell.execute_reply.started":"2025-03-31T07:15:46.766845Z","shell.execute_reply":"2025-03-31T07:15:46.772077Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class LibriSpeechDataset(Dataset): \n    _ext_txt = \".trans.txt\"\n    _ext_audio = \".flac\"\n\n    def __init__(self, data_type='train'):\n        base_path = DATA_PATH\n        if data_type == 'train':\n            self.url = os.path.join(base_path, 'train-clean-100')\n        elif data_type == 'dev':\n            self.url = os.path.join(base_path, 'dev-clean')\n        elif data_type == 'test':\n            self.url = os.path.join(base_path, 'test-clean')\n        else:\n            raise ValueError(\"data_type must be 'train', 'dev', or 'test'\")\n\n        if not os.path.isdir(self.url):\n             raise FileNotFoundError(f\"Dataset directory not found: {self.url}\")\n\n        _dataset_path = Path(self.url).resolve()\n\n        self.walker = []\n        print(f\"Scanning dataset in: {_dataset_path}\")\n        found_files = 0\n        for p in _dataset_path.glob('*/*/*' + self._ext_audio):\n             self.walker.append((str(p.stem), str(_dataset_path)))\n             found_files += 1\n        print(f\"Found {found_files} audio files.\")\n\n        if not self.walker:\n             print(f\"Warning: No audio files found in {self.url} with pattern * G * / * / *{self._ext_audio}\")\n\n        self.walker = sorted(self.walker)\n\n    def __len__(self):\n        return len(self.walker)\n\n    def __getitem__(self, n):\n        fileid, _path = self.walker[n]\n        item = load_librispeech_item(fileid, _path, self._ext_audio, self._ext_txt)\n        while item is None:\n             print(f\"Warning: Skipping item at index {n} due to loading error.\")\n             n = (n + 1) % len(self.walker) \n             if n == self.walker[n][0]: \n                 raise RuntimeError(\"Could not load any valid items from the dataset.\")\n             fileid, _path = self.walker[n]\n             item = load_librispeech_item(fileid, _path, self._ext_audio, self._ext_txt)\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:46.773639Z","iopub.execute_input":"2025-03-31T07:15:46.773894Z","iopub.status.idle":"2025-03-31T07:15:46.793572Z","shell.execute_reply.started":"2025-03-31T07:15:46.773875Z","shell.execute_reply":"2025-03-31T07:15:46.792810Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_set = LibriSpeechDataset('train')\ndev_set = LibriSpeechDataset('dev')\ntest_set = LibriSpeechDataset('test')\nprint(f\"Dataset lengths: Train={len(train_set)}, Dev={len(dev_set)}, Test={len(test_set)}\")\nif len(train_set) == 0 or len(dev_set) == 0 or len(test_set) == 0:\n     print(\"Warning: One or more dataset splits are empty. Check dataset paths and file structure.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:46.795843Z","iopub.execute_input":"2025-03-31T07:15:46.796041Z","iopub.status.idle":"2025-03-31T07:15:47.725105Z","shell.execute_reply.started":"2025-03-31T07:15:46.796024Z","shell.execute_reply":"2025-03-31T07:15:47.724201Z"}},"outputs":[{"name":"stdout","text":"Scanning dataset in: /kaggle/input/librispeech-train-clean-100/train-clean-100\nFound 28539 audio files.\nScanning dataset in: /kaggle/input/librispeech-train-clean-100/dev-clean\nFound 2703 audio files.\nScanning dataset in: /kaggle/input/librispeech-train-clean-100/test-clean\nFound 2620 audio files.\nDataset lengths: Train=28539, Dev=2703, Test=2620\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class DataCollatorSpeechSeq2SeqWithPadding:\n    def __init__(self, feature_extractor, tokenizer):\n        self.feature_extractor = feature_extractor\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        batch = [item for item in batch if item is not None]\n        if not batch:\n             return {}\n\n        waveforms = [item[\"waveform\"] for item in batch]\n        transcripts = [item[\"transcript\"] for item in batch]\n\n        input_features = self.feature_extractor(\n            waveforms,\n            sampling_rate=TARGET_SAMPLE_RATE,\n            return_tensors=\"pt\",\n\n            padding=\"max_length\",  \n            truncation=True,    \n        ).input_features\n\n        labels = self.tokenizer(\n            transcripts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=448\n        ).input_ids\n\n        return {\n            \"input_features\": input_features,\n            \"labels\": labels,\n            \"raw_texts\": transcripts\n        }\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(feature_extractor, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:47.726530Z","iopub.execute_input":"2025-03-31T07:15:47.726871Z","iopub.status.idle":"2025-03-31T07:15:47.732559Z","shell.execute_reply.started":"2025-03-31T07:15:47.726840Z","shell.execute_reply":"2025-03-31T07:15:47.731611Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=data_collator, shuffle=True, num_workers=2, pin_memory=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, collate_fn=data_collator, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=data_collator, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:47.733572Z","iopub.execute_input":"2025-03-31T07:15:47.733904Z","iopub.status.idle":"2025-03-31T07:15:47.766736Z","shell.execute_reply.started":"2025-03-31T07:15:47.733875Z","shell.execute_reply":"2025-03-31T07:15:47.766122Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\nmodel.freeze_encoder()\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:47.767719Z","iopub.execute_input":"2025-03-31T07:15:47.768061Z","iopub.status.idle":"2025-03-31T07:15:48.344580Z","shell.execute_reply.started":"2025-03-31T07:15:47.768031Z","shell.execute_reply":"2025-03-31T07:15:48.343844Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperEncoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51864, 768, padding_idx=50256)\n      (embed_positions): WhisperPositionalEmbedding(448, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperDecoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=768, out_features=51864, bias=False)\n)"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataloader, tokenizer, feature_extractor):\n    print(\"Evaluating...\")\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_refs = []\n    num_batches = 0\n\n    with torch.no_grad():\n        eval_batches = len(dataloader) \n        for i, batch in enumerate(tqdm(dataloader, total=eval_batches, desc=\"Evaluating\")):\n            if i >= eval_batches:\n                 break\n            if not batch: \n                 continue\n\n            input_features = batch[\"input_features\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            raw_texts = batch[\"raw_texts\"]\n            outputs = model(input_features=input_features, labels=labels)\n            loss = outputs.loss\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"Warning: Encountered {loss.item()} loss in evaluation batch {i}. Skipping.\")\n                continue\n            total_loss += loss.item()\n\n            decoder_start_token_id = model.config.decoder_start_token_id\n            decoder_input_ids = torch.full(\n                 (input_features.size(0), 1),\n                 decoder_start_token_id,\n                 dtype=torch.long,\n                 device=device\n            )\n\n            generated_ids = model.generate(\n                input_features,\n                max_length=150 \n            )\n\n            generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            all_preds.extend(generated_texts)\n            all_refs.extend(raw_texts) \n\n            num_batches += 1\n            if num_batches == 0: \n                 print(\"Warning: No valid batches found during evaluation.\")\n                 return float('inf'), float('inf')\n\n\n    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n    normalized_refs = [text.lower() for text in all_refs]\n    normalized_preds = [pred.lower() for pred in all_preds]\n    wer_score = jiwer.wer(normalized_refs, normalized_preds) if all_refs and all_preds else float('inf')\n\n    print(f\"Evaluation Results - Loss: {avg_loss:.4f}, WER: {wer_score:.4f}\")\n    print(\"Example predictions:\")\n    for i in range(min(3, len(all_refs))):\n         print(f\"  Ref: {all_refs[i]}\")\n         print(f\"  Pred: {all_preds[i]}\")\n         print(\"-\" * 10)\n\n    model.train() \n    return avg_loss, wer_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:48.345328Z","iopub.execute_input":"2025-03-31T07:15:48.345615Z","iopub.status.idle":"2025-03-31T07:15:48.354252Z","shell.execute_reply.started":"2025-03-31T07:15:48.345593Z","shell.execute_reply":"2025-03-31T07:15:48.353435Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Optimizer","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE) # AdamW thường tốt hơn Adam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:48.355042Z","iopub.execute_input":"2025-03-31T07:15:48.355270Z","iopub.status.idle":"2025-03-31T07:15:48.375394Z","shell.execute_reply.started":"2025-03-31T07:15:48.355251Z","shell.execute_reply":"2025-03-31T07:15:48.374815Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"steps = 0\ngradient_steps = 0\ntotal_loss_accumulated = 0.0\n\nos.makedirs(SAVE_TO, exist_ok=True)\n\nlosses = []\nepoch_train_losses = []\nval_losses = []\nval_wers = []\n\ncurrent_val_loss = float('inf')\ncurrent_val_wer = float('inf')\n# val_loss, val_wer = evaluate(model, dev_loader, tokenizer, feature_extractor)\n# print(f\"Initial Eval - Loss: {val_loss:.4f}, WER: {val_wer:.4f}\")\n# val_losses.append(val_loss)\n# val_wers.append(val_wer)\n\nmodel.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:15:48.376589Z","iopub.execute_input":"2025-03-31T07:15:48.376906Z","iopub.status.idle":"2025-03-31T07:27:47.139769Z","shell.execute_reply.started":"2025-03-31T07:15:48.376878Z","shell.execute_reply":"2025-03-31T07:27:47.138855Z"}},"outputs":[{"name":"stdout","text":"Evaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/169 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3d650a61ce4275859ae10f321dbddf"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Evaluation Results - Loss: 0.0543, WER: 0.0437\nExample predictions:\n  Ref: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n  Pred: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n----------\n  Ref: NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n  Pred: NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n----------\n  Ref: HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\n  Pred: HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILIES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\n----------\nInitial Eval - Loss: 0.0543, WER: 0.0437\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperEncoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51864, 768, padding_idx=50256)\n      (embed_positions): WhisperPositionalEmbedding(448, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperDecoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=768, out_features=51864, bias=False)\n)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"\\n--- Epoch: {epoch + 1}/{EPOCHS} ---\")\n    model.train() \n    epoch_loss_sum = 0.0\n    batches_in_epoch = 0\n\n    training_loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n\n    for batch in training_loop:\n        if not batch: \n            continue\n\n        input_features = batch[\"input_features\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_features=input_features, labels=labels)\n        loss = outputs.loss\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"Warning: Encountered {loss.item()} loss at step {steps}. Skipping batch.\")\n            optimizer.zero_grad()\n            continue\n\n        epoch_loss_sum += loss.item() * GRADIENT_ACCUMULATION_STEPS # Tính lại loss gốc trước khi scale\n        batches_in_epoch += 1\n\n        loss = loss / GRADIENT_ACCUMULATION_STEPS\n        total_loss_accumulated += loss.item() # Cộng loss đã scale để tính loss của gradient step\n        loss.backward()\n        \n        steps += 1\n        if steps % GRADIENT_ACCUMULATION_STEPS == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n            gradient_steps += 1\n\n            avg_accumulated_loss = total_loss_accumulated \n            losses.append(avg_accumulated_loss * GRADIENT_ACCUMULATION_STEPS) \n            total_loss_accumulated = 0.0 \n\n            training_loop.set_description(\n                 f\"Epoch {epoch + 1} | Step: {steps} | Grad Steps: {gradient_steps} | Batch Loss: {loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f} | Val Loss: {current_val_loss:.4f} | Val WER: {current_val_wer:.4f}\"\n            )\n\n    avg_epoch_train_loss = epoch_loss_sum / batches_in_epoch if batches_in_epoch > 0 else float('inf')\n    epoch_train_losses.append(avg_epoch_train_loss)\n    print(f\"Epoch {epoch + 1} finished. Average Training Loss: {avg_epoch_train_loss:.4f}\")\n\n    print(f\"Evaluating at the end of Epoch {epoch + 1}...\")\n    current_val_loss, current_val_wer = evaluate(model, dev_loader, tokenizer, feature_extractor)\n    val_losses.append(current_val_loss)\n    val_wers.append(current_val_wer)\n    print(f\"Epoch {epoch + 1} Eval - Loss: {current_val_loss:.4f}, WER: {current_val_wer:.4f}\")\n\n    if (epoch + 1) % CHECKPOINT_EVERY_N_EPOCHS == 0 or (epoch + 1) == EPOCHS:\n        print(f\"\\nSaving checkpoint at the end of Epoch {epoch + 1}...\")\n        save_path = os.path.join(SAVE_TO, f'whisper-epoch-{epoch + 1}')\n        model.save_pretrained(save_path)\n        tokenizer.save_pretrained(save_path)\n        feature_extractor.save_pretrained(save_path)\n        print(f\"Checkpoint saved to {save_path}\")\n\n        try:\n            with open(os.path.join(SAVE_TO, f'losses_grad_step_epoch_{epoch+1}.pkl'), 'wb') as f:\n                 pickle.dump(losses, f)\n            with open(os.path.join(SAVE_TO, f'epoch_train_losses_epoch_{epoch+1}.pkl'), 'wb') as f:\n                 pickle.dump(epoch_train_losses, f)\n            with open(os.path.join(SAVE_TO, f'val_losses_epoch_{epoch+1}.pkl'), 'wb') as f:\n                 pickle.dump(val_losses, f)\n            with open(os.path.join(SAVE_TO, f'val_wers_epoch_{epoch+1}.pkl'), 'wb') as f:\n                 pickle.dump(val_wers, f)\n            print(\"Metrics saved for Epoch\", epoch + 1)\n        except Exception as e:\n            print(f\"Error saving metrics for Epoch {epoch + 1}: {e}\")\n\n    model.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final review after completing all epochs","metadata":{}},{"cell_type":"code","source":"print(\"--- Training Finished ---\")\nprint(\"Performing final evaluation on Dev set...\")\nfinal_val_loss, final_val_wer = evaluate(model, dev_loader, tokenizer, feature_extractor)\nprint(f\"Final Dev Eval - Loss: {final_val_loss:.4f}, WER: {final_val_wer:.4f}\")\n\nprint(\"Performing final evaluation on Test set...\")\nfinal_test_loss, final_test_wer = evaluate(model, test_loader, tokenizer, feature_extractor)\nprint(f\"Final Test Eval - Loss: {final_test_loss:.4f}, WER: {final_test_wer:.4f}\")\nprint(\"Saving final model...\")\nfinal_save_path = os.path.join(SAVE_TO, 'whisper-final')\nmodel.save_pretrained(final_save_path)\ntokenizer.save_pretrained(final_save_path)\nfeature_extractor.save_pretrained(final_save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T07:37:15.430799Z","iopub.execute_input":"2025-03-31T07:37:15.431128Z","iopub.status.idle":"2025-03-31T08:01:28.722476Z","shell.execute_reply.started":"2025-03-31T07:37:15.431101Z","shell.execute_reply":"2025-03-31T08:01:28.721627Z"}},"outputs":[{"name":"stdout","text":"--- Training Finished ---\nPerforming final evaluation on Dev set...\nEvaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/169 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79f48582448741c7a15decc44c045323"}},"metadata":{}},{"name":"stdout","text":"Evaluation Results - Loss: 0.0567, WER: 0.0499\nExample predictions:\n  Ref: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n  Pred: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n----------\n  Ref: NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n  Pred: NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n----------\n  Ref: HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\n  Pred:  HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILIES DRAWN FROM EATING AND ITS RESULTS OCCURMOSE READILY TO THE MINE\n----------\nFinal Dev Eval - Loss: 0.0567, WER: 0.0499\nPerforming final evaluation on Test set...\nEvaluating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/164 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49fa9379af8740009000e3227d48e1ad"}},"metadata":{}},{"name":"stdout","text":"Evaluation Results - Loss: 0.0563, WER: 0.0487\nExample predictions:\n  Ref: HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE\n  Pred: HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE\n----------\n  Ref: STUFF IT INTO YOU HIS BELLY COUNSELLED HIM\n  Pred: STUFF IT INTO YOU HIS BELLY COUNSELLED HIM\n----------\n  Ref: AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n  Pred: AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n----------\nFinal Test Eval - Loss: 0.0563, WER: 0.0487\nSaving final model...\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/models/whisper-final/preprocessor_config.json']"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"with open(os.path.join(SAVE_TO, 'losses.pkl'), 'wb') as f:\n    pickle.dump(losses, f)\nwith open(os.path.join(SAVE_TO, 'val_losses.pkl'), 'wb') as f:\n    pickle.dump(val_losses + [final_val_loss], f)\nwith open(os.path.join(SAVE_TO, 'val_wers.pkl'), 'wb') as f:\n    pickle.dump(val_wers + [final_val_wer], f)\nwith open(os.path.join(SAVE_TO, 'test_results.pkl'), 'wb') as f:\n    pickle.dump({'loss': final_test_loss, 'wer': final_test_wer}, f)\nprint(\"Final model and metrics saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:14:53.765876Z","iopub.execute_input":"2025-03-31T08:14:53.766229Z","iopub.status.idle":"2025-03-31T08:14:53.773258Z","shell.execute_reply.started":"2025-03-31T08:14:53.766195Z","shell.execute_reply":"2025-03-31T08:14:53.772395Z"}},"outputs":[{"name":"stdout","text":"Final model and metrics saved.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"losses, val_losses+[final_val_loss], val_wers + [final_val_wer], final_test_loss, final_test_wer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:16:22.026375Z","iopub.execute_input":"2025-03-31T08:16:22.026689Z","iopub.status.idle":"2025-03-31T08:16:22.031945Z","shell.execute_reply.started":"2025-03-31T08:16:22.026663Z","shell.execute_reply":"2025-03-31T08:16:22.031247Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"([0.08220819412963465,\n  1.4517763387411833,\n  1.4123799963854253,\n  0.6676259930245578,\n  0.9075691448524594],\n [0.05432921249595796, 0.0566839841139343],\n [0.04365648321752877, 0.049924635123708684],\n 0.05625625495321867,\n 0.048691418137553254)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}